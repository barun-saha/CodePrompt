llm = HuggingFaceHub(
    repo_id=LLM_MODEL_NAME,
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
    model_kwargs={'temperature': 0.1, 'max_length': 100}
)
# print(llm)

def generate_summary(poem: str) -> str:
    """
    Generate a summary of the given poem.

    Args:
        poem (str): The poem to summarize.

    Returns:
        str: The summary of the poem.
    """
    print(prompt.format(poem=poem))
    summary = llm(prompt.format(poem=poem))

    return summary


def generate_image_from_text(text: str) -> PIL.Image.Image:
    """
    Generate an image from the given text.

    Args:
        text (str): The text to generate an image from.

    Returns:
        PIL.Image.Image: The generated image.
    """
    with torch.inference_mode():
        output_img = pipeline(
            text,
            num_inference_steps=Config.DIFFUSION_NUM_INFERENCE_STEPS).images[0]
        print(output_img)

        return output_img
```
